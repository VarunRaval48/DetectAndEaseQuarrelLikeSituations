{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas\n",
    "import json\n",
    "import nltk.data\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import string\n",
    "import array\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score, fbeta_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tags(sentence):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_files(sources):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for source in sources:\n",
    "        print(source)\n",
    "        sourceJ = source[0]\n",
    "        sourceT = source[1]\n",
    "        for root, dir_names, file_names in os.walk(sourceJ):\n",
    "            for file_name in file_names:\n",
    "                try:\n",
    "                    file = open(os.path.join(sourceT, file_name[:-5]+'.txt'))\n",
    "                    content = tokenizer.tokenize(file.read())\n",
    "                except:\n",
    "                    file = open(os.path.join(sourceT, file_name[:-5]+'.txt'), encoding='windows-1252')\n",
    "                    content = tokenizer.tokenize(file.read())\n",
    "                vals = []\n",
    "                encoding='utf-8'\n",
    "                while True:\n",
    "                    try:\n",
    "                        for line in open(os.path.join(root, file_name), encoding=encoding):\n",
    "                            for node in json.loads(line)['nodes']:\n",
    "                                vals.append(node['text'])\n",
    "                        break\n",
    "                    except:\n",
    "                        encoding='windows-1252'\n",
    "\n",
    "                r_val = [1]*len(vals)\n",
    "                args= []\n",
    "                non_args = []\n",
    "                pos_args = []\n",
    "                pos_non_args = []\n",
    "                for con in content:\n",
    "                    is_args = False\n",
    "                    i = 0\n",
    "                    for val in vals:\n",
    "                        if val in con:\n",
    "                            is_args = True\n",
    "                            r_val[i] = 0\n",
    "                            break\n",
    "                        i+=1\n",
    "                    pos_con = nltk.pos_tag(nltk.word_tokenize(con))\n",
    "                    if(is_args):\n",
    "                        args.append(con)\n",
    "                        pos_args.append(pos_con)\n",
    "                    else:\n",
    "                        non_args.append(con)\n",
    "                        pos_non_args.append(pos_con)\n",
    "\n",
    "                i = 0\n",
    "                for val in vals:\n",
    "                    if(r_val[i] and val!='RA'):\n",
    "                        pos_val = nltk.pos_tag(nltk.word_tokenize(val))\n",
    "                        args.append(val)\n",
    "                        pos_args.append(pos_val)\n",
    "                    i+=1\n",
    "\n",
    "                yield file_name, args, non_args, pos_args, pos_non_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file = open('output.txt', 'w')\n",
    "for file_name, args, non_args in read_files():\n",
    "    s = file_name, ':', args, ':', non_args\n",
    "    file.write(str(s)+\"\\n\")\n",
    "\"\"\"\n",
    "def get_df(sources):\n",
    "    headings = ['arguments', 'non arguments']\n",
    "    index = []\n",
    "    data = []\n",
    "    pos_data = []\n",
    "    for file_name, args, non_args, pos_args, pos_non_args in read_files(sources):\n",
    "        index.append(file_name)\n",
    "        data.append([args, non_args])\n",
    "        pos_data.append([pos_args, pos_non_args])\n",
    "\n",
    "    df = pandas.DataFrame(index = index, data = data, columns = headings)\n",
    "    pos_df = pandas.DataFrame(index = index, data = pos_data, columns = headings)\n",
    "    return df, pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/araucaria/json', 'data/araucaria/txt']\n"
     ]
    }
   ],
   "source": [
    "sources = [['data/araucaria/json', 'data/araucaria/txt']]\n",
    "df, pos_df = get_df(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adverbs = ['RB', 'RBR', 'RBS']\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "verbs_remove = ['to be', 'to do', 'to have']\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _make_int_array():\n",
    "    return array.array(str(\"i\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _ngrams(tokens):\n",
    "    o_tokens = tokens\n",
    "    tokens = []\n",
    "    n_o_tokens = len(o_tokens)\n",
    "    \n",
    "    for n in range(2,4):\n",
    "        for j in range(n_o_tokens - n + 1):\n",
    "            yield ' '.join(o_tokens[j:j+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _couples(tokens):\n",
    "    length = len(tokens)\n",
    "    for i in range(length-1):\n",
    "        for j in range(i+1, length):\n",
    "            yield 'c_{} {}'.format(tokens[i], tokens[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _adverbs(pos_tags):\n",
    "    p = False\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in adverbs:\n",
    "            p = True\n",
    "            yield 'adv_' + word\n",
    "\n",
    "    if not p:\n",
    "        yield 'no_adverbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _verbs(pos_tags):\n",
    "    p = False\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in verbs and word not in verbs_remove:\n",
    "            p = True\n",
    "            yield 'v_' + word\n",
    "            \n",
    "    if not p:\n",
    "        yield 'no_verbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _modal_aux(pos_tags):\n",
    "    for word, tag in pos_tags:\n",
    "        if tag == 'MD':\n",
    "            return 'MD_1', 1\n",
    "    return 'MD_1', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sen_len(tokens):\n",
    "    yield 's_len', len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _avg_word_len(tokens):\n",
    "    c_len = 0\n",
    "    for token in tokens:\n",
    "        c_len += len(token)\n",
    "    yield 'avg_word_len', round(c_len/len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _punc_data(s):\n",
    "    punc_list = list(filter(lambda c: c in s, string.punctuation))\n",
    "    punc_len = len(punc_list)\n",
    "    yield 'punc_len', punc_len\n",
    "    single_punc = []\n",
    "    i = 0\n",
    "    while i < punc_len:\n",
    "        single_punc.append(punc_list[i])\n",
    "        last = punc_list[i]\n",
    "        i += 1\n",
    "        while(i < punc_len and punc_list[i] == last):\n",
    "            i += 1\n",
    "            \n",
    "    yield ''.join(single_punc), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new():\n",
    "    global features_\n",
    "    features_ = defaultdict()\n",
    "    features_.default_factory = features_.__len__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _feature_dict(sentence, pos_tags, fixed_features):\n",
    "    global features_\n",
    "    typeF = ''\n",
    "    if fixed_features == True:\n",
    "        features = dict(features_)\n",
    "        typeF = 'dict'\n",
    "    else:\n",
    "        features = features_\n",
    "        typeF = 'defaultDict'\n",
    "\n",
    "    feature_dict = {}\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    if len(tokens) > 2:\n",
    "#         pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        tag, val = _modal_aux(pos_tags)\n",
    "        if typeF == 'dict':\n",
    "            if tag in features:\n",
    "                feature_num = features[tag]\n",
    "                feature_dict[feature_num] = val\n",
    "        else:\n",
    "            feature_num = features[tag]\n",
    "            feature_dict[feature_num] = val\n",
    "\n",
    "        for tag, val in itertools.chain(_sen_len(tokens), _avg_word_len(tokens), _punc_data(sentence)):\n",
    "            if typeF == 'dict':\n",
    "                if tag in features:\n",
    "                    feature_num = features[tag]\n",
    "                    feature_dict[feature_num] = val\n",
    "            else:\n",
    "                feature_num = features[tag]\n",
    "                feature_dict[feature_num] = val\n",
    "\n",
    "        for token in itertools.chain(_adverbs(pos_tags), _verbs(pos_tags), tokens, _ngrams(tokens), _couples(tokens)):\n",
    "            if typeF == 'dict':\n",
    "                if token in features:\n",
    "                    feature_num = features[token]\n",
    "                    if feature_num not in feature_dict:\n",
    "                        feature_dict[feature_num] = 1\n",
    "                    else:\n",
    "                        feature_dict[feature_num] += 1\n",
    "            else:\n",
    "                feature_num = features[token]\n",
    "                if feature_num not in feature_dict:\n",
    "                    feature_dict[feature_num] = 1\n",
    "                else:\n",
    "                    feature_dict[feature_num] += 1\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(feature_dict):\n",
    "    global features_\n",
    "    features = np.zeros(len(features_))\n",
    "    for key in feature_dict:\n",
    "        features[key] = feature_dict[key]\n",
    "    \n",
    "    return features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_transform(dataFrame, pos_dataFrame, fixed_features=False):\n",
    "    global features_\n",
    "    j_indices = []\n",
    "    indptr = _make_int_array()\n",
    "    values = []\n",
    "    indptr.append(0)\n",
    "    targets = []\n",
    "    for i, heading in enumerate(headings):\n",
    "        for j, content in enumerate(dataFrame[heading]):\n",
    "            for k, sentence in enumerate(content):\n",
    "                feature_dict = _feature_dict(sentence, pos_dataFrame[heading][j][k], fixed_features)\n",
    "                # print(sentence, feature_dict)\n",
    "                if len(feature_dict) != 0:\n",
    "                    targets.append(i)\n",
    "                    j_indices.extend(feature_dict.keys())\n",
    "                    values.extend(feature_dict.values())\n",
    "                    indptr.append(len(j_indices))\n",
    "\n",
    "    j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "    indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "    targets = np.asarray(targets, dtype=np.intc)\n",
    "    X = csr_matrix((values, j_indices, indptr), shape = (len(indptr) - 1, len(features_)), dtype=np.int64)\n",
    "    return X, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initialize_new()\n",
    "X, targets = fit_transform(df, pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 701401)\n",
      "[0 0 0 0 0]\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 701401)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# j_indices = []\n",
    "# values = []\n",
    "# indptr = _make_int_array()\n",
    "# indptr.append(0)\n",
    "\n",
    "# feature_dict = _feature_dict('Indonesia cannot afford to become a haven for an Islamic radicalism which would wreck the economy and tear the archipelago apart.')\n",
    "# j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "# indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "# targets = np.asarray(targets, dtype=np.intc)\n",
    "\n",
    "X, targets = fit_transform(df.iloc[[0]], pos_df.iloc[[0]], fixed_features=True)\n",
    "print(X.shape)\n",
    "print(classifier.predict(X))\n",
    "\n",
    "sentence = 'We now know that life-prohibiting universes are vastly more probable than life- permitting universes like ours.'\n",
    "features = get_features(_feature_dict(sentence, get_pos_tags(sentence), fixed_features=True))\n",
    "print(classifier.predict(features))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kFoldTest(data, pos_data, classifierT = 'multinomialNB'):\n",
    "#     pipeline = Pipeline([\n",
    "#             ('vectorizer', Vectorize()),\n",
    "#             ('classifier', classifier)\n",
    "#         ])\n",
    "\n",
    "    kFold = KFold(n = len(data), n_folds = 4)\n",
    "    scores = []\n",
    "    p_scores = []\n",
    "    r_scores = []\n",
    "    confusionMatrix = np.array([[0]*2]*2)\n",
    "#     vectorizer = Vectorize()\n",
    "    #classifier = MultinomialNB()\n",
    "    #classifier = BernoulliNB()\n",
    "    #classifier = SVC()\n",
    "    #classifier = RandomForestClassifier()\n",
    "    for train_indices, test_indices in kFold:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        test_data = data.iloc[test_indices]\n",
    "        train_pos_data = pos_data.iloc[train_indices]\n",
    "        test_pos_data = pos_data.iloc[test_indices]\n",
    "\n",
    "        if(classifierT == 'multinomialNB'):\n",
    "            classifier = MultinomialNB()\n",
    "        elif(classifierT == 'maxent'):\n",
    "            classifier = LogisticRegression()\n",
    "\n",
    "        initialize_new()\n",
    "\n",
    "        X, targets = fit_transform(train_data, train_pos_data)\n",
    "        train_y = targets\n",
    "        classifier.fit(X, train_y)\n",
    "\n",
    "        print(X.shape)\n",
    "        \n",
    "        X, targets = fit_transform(test_data, test_pos_data, fixed_features=True)\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        predictions = classifier.predict(X)\n",
    "        test_y = targets\n",
    "\n",
    "        # print(test_y.shape)\n",
    "        # print(predictions.shape)\n",
    "        # print(confusion_matrix(test_y, predictions))\n",
    "        confusionMatrix += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, average='binary')\n",
    "        p_score = precision_score(test_y, predictions, average='binary')\n",
    "        r_score = recall_score(test_y, predictions, average='binary')\n",
    "        \n",
    "        p_scores.append(p_score)\n",
    "        r_scores.append(r_score)        \n",
    "        scores.append(score)\n",
    "\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Precision Score:', sum(p_scores)/len(p_scores))\n",
    "    print('Recall Score:', sum(r_scores)/len(r_scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 583323)\n",
      "(1565, 583323)\n",
      "(4861, 580952)\n",
      "(1682, 580952)\n",
      "(4868, 563002)\n",
      "(1675, 563002)\n",
      "(4922, 573279)\n",
      "(1621, 573279)\n",
      "Score: 0.73806027188\n",
      "Precision Score: 0.767051730881\n",
      "Recall Score: 0.71167835531\n",
      "Confusion matrix:\n",
      "[[3484  543]\n",
      " [ 727 1789]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(df, pos_df, 'multinomialNB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 583322)\n",
      "(1565, 583322)\n",
      "(4861, 580951)\n",
      "(1682, 580951)\n",
      "(4868, 563002)\n",
      "(1675, 563002)\n",
      "(4922, 573278)\n",
      "(1621, 573278)\n",
      "Score: 0.825608135503\n",
      "Precision Score: 0.827634422406\n",
      "Recall Score: 0.823856576752\n",
      "Confusion matrix:\n",
      "[[3596  431]\n",
      " [ 444 2072]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(df, classifierT='maxent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using this classifier on another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sources = ['data/schemes/txt']\n",
    "def classify_data(sources):\n",
    "    index = []\n",
    "    data = []\n",
    "    for source in sources:\n",
    "        for root, dir_names, file_names in os.walk(source):\n",
    "            for file_name in file_names:\n",
    "                try:\n",
    "                    file = open(os.path.join(root, file_name))\n",
    "                    content = tokenizer.tokenize(file.read())\n",
    "                except:\n",
    "                    file = open(os.path.join(root, file_name), encoding='windows-1252')\n",
    "                    content = tokenizer.tokenize(file.read())\n",
    "\n",
    "                lines = []\n",
    "                for line in content:\n",
    "                    features = get_features(_feature_dict(line, get_pos_tags(line), fixed_features=True))\n",
    "                    result = classifier.predict(features)\n",
    "                    lines.append([line, result])\n",
    "\n",
    "                index.append(source + \"/\" + file_name)\n",
    "                data.append(lines)\n",
    "\n",
    "    df = pandas.DataFrame(index=index, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
