{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas\n",
    "import json\n",
    "import nltk.data\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import string\n",
    "import array\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score, fbeta_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tags(sentence):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_files(sources):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for source in sources:\n",
    "        print(source)\n",
    "        sourceJ = source[0]\n",
    "        sourceT = source[1]\n",
    "        for root, dir_names, file_names in os.walk(sourceJ):\n",
    "            for file_name in file_names:\n",
    "                try:\n",
    "                    file = open(os.path.join(sourceT, file_name[:-5]+'.txt'))\n",
    "                    content = tokenizer.tokenize(file.read())\n",
    "                except:\n",
    "                    file = open(os.path.join(sourceT, file_name[:-5]+'.txt'), encoding='windows-1252')\n",
    "                    content = tokenizer.tokenize(file.read())\n",
    "                vals = []\n",
    "                encoding='utf-8'\n",
    "                while True:\n",
    "                    try:\n",
    "                        for line in open(os.path.join(root, file_name), encoding=encoding):\n",
    "                            for node in json.loads(line)['nodes']:\n",
    "                                vals.append(node['text'])\n",
    "                        break\n",
    "                    except:\n",
    "                        encoding='windows-1252'\n",
    "\n",
    "                r_val = [1]*len(vals)\n",
    "                args= []\n",
    "                non_args = []\n",
    "                pos_args = []\n",
    "                pos_non_args = []\n",
    "                for con in content:\n",
    "                    is_args = False\n",
    "                    i = 0\n",
    "                    for val in vals:\n",
    "                        if val in con:\n",
    "                            is_args = True\n",
    "                            r_val[i] = 0\n",
    "                            break\n",
    "                        i+=1\n",
    "                    pos_con = nltk.pos_tag(nltk.word_tokenize(con))\n",
    "                    if(is_args):\n",
    "                        args.append(con)\n",
    "                        pos_args.append(pos_con)\n",
    "                    else:\n",
    "                        non_args.append(con)\n",
    "                        pos_non_args.append(pos_con)\n",
    "\n",
    "                i = 0\n",
    "                for val in vals:\n",
    "                    if(r_val[i] and val!='RA'):\n",
    "                        pos_val = nltk.pos_tag(nltk.word_tokenize(val))\n",
    "                        args.append(val)\n",
    "                        pos_args.append(pos_val)\n",
    "                    i+=1\n",
    "\n",
    "                yield file_name, args, non_args, pos_args, pos_non_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file = open('output.txt', 'w')\n",
    "for file_name, args, non_args in read_files():\n",
    "    s = file_name, ':', args, ':', non_args\n",
    "    file.write(str(s)+\"\\n\")\n",
    "\"\"\"\n",
    "def get_df(sources):\n",
    "    headings = ['arguments', 'non arguments']\n",
    "    index = []\n",
    "    data = []\n",
    "    pos_data = []\n",
    "    for file_name, args, non_args, pos_args, pos_non_args in read_files(sources):\n",
    "        index.append(file_name)\n",
    "        data.append([args, non_args])\n",
    "        pos_data.append([pos_args, pos_non_args])\n",
    "\n",
    "    df = pandas.DataFrame(index = index, data = data, columns = headings)\n",
    "    pos_df = pandas.DataFrame(index = index, data = pos_data, columns = headings)\n",
    "    return df, pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/araucaria/json', 'data/araucaria/txt']\n"
     ]
    }
   ],
   "source": [
    "sources = [['data/araucaria/json', 'data/araucaria/txt']]\n",
    "df, pos_df = get_df(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adverbs = ['RB', 'RBR', 'RBS']\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "verbs_remove = ['to be', 'to do', 'to have']\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _make_int_array():\n",
    "    return array.array(str(\"i\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _ngrams(tokens):\n",
    "    o_tokens = tokens\n",
    "    tokens = []\n",
    "    n_o_tokens = len(o_tokens)\n",
    "    \n",
    "    for n in range(2,4):\n",
    "        for j in range(n_o_tokens - n + 1):\n",
    "            yield ' '.join(o_tokens[j:j+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _couples(tokens):\n",
    "    length = len(tokens)\n",
    "    for i in range(length-1):\n",
    "        for j in range(i+1, length):\n",
    "            yield 'c_{} {}'.format(tokens[i], tokens[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _adverbs(pos_tags):\n",
    "    p = False\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in adverbs:\n",
    "            p = True\n",
    "            yield 'adv_' + word\n",
    "\n",
    "    if not p:\n",
    "        yield 'no_adverbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _verbs(pos_tags):\n",
    "    p = False\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in verbs and word not in verbs_remove:\n",
    "            p = True\n",
    "            yield 'v_' + word\n",
    "            \n",
    "    if not p:\n",
    "        yield 'no_verbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _modal_aux(pos_tags):\n",
    "    for word, tag in pos_tags:\n",
    "        if tag == 'MD':\n",
    "            return 'MD_1', 1\n",
    "    return 'MD_1', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sen_len(tokens):\n",
    "    yield 's_len', len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _avg_word_len(tokens):\n",
    "    c_len = 0\n",
    "    for token in tokens:\n",
    "        c_len += len(token)\n",
    "    yield 'avg_word_len', round(c_len/len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _punc_data(s):\n",
    "    punc_list = list(filter(lambda c: c in s, string.punctuation))\n",
    "    punc_len = len(punc_list)\n",
    "    yield 'punc_len', punc_len\n",
    "    single_punc = []\n",
    "    i = 0\n",
    "    while i < punc_len:\n",
    "        single_punc.append(punc_list[i])\n",
    "        last = punc_list[i]\n",
    "        i += 1\n",
    "        while(i < punc_len and punc_list[i] == last):\n",
    "            i += 1\n",
    "            \n",
    "    yield ''.join(single_punc), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new():\n",
    "    global features_\n",
    "    features_ = defaultdict()\n",
    "    features_.default_factory = features_.__len__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _feature_dict(sentence, pos_tags, fixed_features):\n",
    "    global features_\n",
    "    typeF = ''\n",
    "    if fixed_features == True:\n",
    "        features = dict(features_)\n",
    "        typeF = 'dict'\n",
    "    else:\n",
    "        features = features_\n",
    "        typeF = 'defaultDict'\n",
    "\n",
    "    feature_dict = {}\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    if len(tokens) > 2:\n",
    "#         pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        tag, val = _modal_aux(pos_tags)\n",
    "        if typeF == 'dict':\n",
    "            if tag in features:\n",
    "                feature_num = features[tag]\n",
    "                feature_dict[feature_num] = val\n",
    "        else:\n",
    "            feature_num = features[tag]\n",
    "            feature_dict[feature_num] = val\n",
    "\n",
    "        for tag, val in itertools.chain(_sen_len(tokens), _avg_word_len(tokens), _punc_data(sentence)):\n",
    "            if typeF == 'dict':\n",
    "                if tag in features:\n",
    "                    feature_num = features[tag]\n",
    "                    feature_dict[feature_num] = val\n",
    "            else:\n",
    "                feature_num = features[tag]\n",
    "                feature_dict[feature_num] = val\n",
    "\n",
    "        for token in itertools.chain(_adverbs(pos_tags), _verbs(pos_tags), tokens, _ngrams(tokens), _couples(tokens)):\n",
    "            if typeF == 'dict':\n",
    "                if token in features:\n",
    "                    feature_num = features[token]\n",
    "                    if feature_num not in feature_dict:\n",
    "                        feature_dict[feature_num] = 1\n",
    "                    else:\n",
    "                        feature_dict[feature_num] += 1\n",
    "            else:\n",
    "                feature_num = features[token]\n",
    "                if feature_num not in feature_dict:\n",
    "                    feature_dict[feature_num] = 1\n",
    "                else:\n",
    "                    feature_dict[feature_num] += 1\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(feature_dict):\n",
    "    global features_\n",
    "    features = np.zeros(len(features_))\n",
    "    for key in feature_dict:\n",
    "        features[key] = feature_dict[key]\n",
    "    \n",
    "    return features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_transform(dataFrame, pos_dataFrame, fixed_features=False):\n",
    "    global features_\n",
    "    j_indices = []\n",
    "    indptr = _make_int_array()\n",
    "    values = []\n",
    "    indptr.append(0)\n",
    "    targets = []\n",
    "    for i, heading in enumerate(headings):\n",
    "        for j, content in enumerate(dataFrame[heading]):\n",
    "            for k, sentence in enumerate(content):\n",
    "                feature_dict = _feature_dict(sentence, pos_dataFrame[heading][j][k], fixed_features)\n",
    "                # print(sentence, feature_dict)\n",
    "                if len(feature_dict) != 0:\n",
    "                    targets.append(i)\n",
    "                    j_indices.extend(feature_dict.keys())\n",
    "                    values.extend(feature_dict.values())\n",
    "                    indptr.append(len(j_indices))\n",
    "\n",
    "    j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "    indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "    targets = np.asarray(targets, dtype=np.intc)\n",
    "    X = csr_matrix((values, j_indices, indptr), shape = (len(indptr) - 1, len(features_)), dtype=np.int64)\n",
    "    return X, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initialize_new()\n",
    "X, targets = fit_transform(df, pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6543"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6543, 701401)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 23,  5, ...,  1,  1,  1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arguments        [Indonesia cannot afford to become a haven for...\n",
       "non arguments                                                   []\n",
       "Name: nodeset190.json, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df.iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indonesia cannot afford to become a haven for an Islamic radicalism which would wreck the economy and tear the archipelago apart.',\n",
       " 'The war on terror will be intensified.',\n",
       " 'If Indonesia wants to avoid becoming a haven for Islamic radicalism the war on terror has to be intensified',\n",
       " 'There is a possibility for Indonesia to become a haven for Islamic radicalism',\n",
       " 'which would wreck the economy and tear the archipelago apart.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['arguments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{8} During the last thirty years, scientists have discovered that the existence of intelligent life depends upon a complex and delicately balanced set of initial conditions simply given in the Big Bang itself.',\n",
       " 'We now know that life-prohibiting universes are vastly more probable than life- permitting universes like ours.',\n",
       " 'How much more probable?',\n",
       " 'Well, before I give you an estimation, let me just give you some numbers to give you a feel for the odds.',\n",
       " \"The number of seconds in the history of the universe is about 1018, that's ten followed by eighteen zeros.\",\n",
       " 'The number of subatomic particles in the entire universe is about1080.',\n",
       " 'Now with those numbers in mind, consider the following.',\n",
       " \"Donald Page, one of America's eminent cosmologists, has calculated the odds of our universe existing as on the order of one chance out of 1010(123), a number which is so inconceivable that to call it astronomical would be a wild understatement!\",\n",
       " '{9} Robert Jastrow, the head of NASA\\'s Goddard Institute for Space Studies, has called this the most powerful evidence for the existence of God \"ever to come out of science.',\n",
       " '\"{10} Once again, the view that Christian theists have always held, that there is an intelligent designer of the Cosmos, seems to me to be much more plausible than the atheistic interpretation of chance.',\n",
       " 'http://www.origins.org/articles/craig_washington_3.html']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['non arguments'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 701401)\n",
      "[0 0 0 0 0]\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 701401)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# j_indices = []\n",
    "# values = []\n",
    "# indptr = _make_int_array()\n",
    "# indptr.append(0)\n",
    "\n",
    "# feature_dict = _feature_dict('Indonesia cannot afford to become a haven for an Islamic radicalism which would wreck the economy and tear the archipelago apart.')\n",
    "# j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "# indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "# targets = np.asarray(targets, dtype=np.intc)\n",
    "\n",
    "X, targets = fit_transform(df.iloc[[0]], pos_df.iloc[[0]], fixed_features=True)\n",
    "print(X.shape)\n",
    "print(classifier.predict(X))\n",
    "\n",
    "sentence = 'We now know that life-prohibiting universes are vastly more probable than life- permitting universes like ours.'\n",
    "features = get_features(_feature_dict(sentence, get_pos_tags(sentence), fixed_features=True))\n",
    "print(classifier.predict(features))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 701402)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kFoldTest(data, pos_data, classifierT = 'multinomialNB'):\n",
    "#     pipeline = Pipeline([\n",
    "#             ('vectorizer', Vectorize()),\n",
    "#             ('classifier', classifier)\n",
    "#         ])\n",
    "\n",
    "    kFold = KFold(n = len(data), n_folds = 4)\n",
    "    scores = []\n",
    "    p_scores = []\n",
    "    r_scores = []\n",
    "    confusionMatrix = np.array([[0]*2]*2)\n",
    "#     vectorizer = Vectorize()\n",
    "    #classifier = MultinomialNB()\n",
    "    #classifier = BernoulliNB()\n",
    "    #classifier = SVC()\n",
    "    #classifier = RandomForestClassifier()\n",
    "    for train_indices, test_indices in kFold:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        test_data = data.iloc[test_indices]\n",
    "        train_pos_data = pos_data.iloc[train_indices]\n",
    "        test_pos_data = pos_data.iloc[test_indices]\n",
    "\n",
    "        if(classifierT == 'multinomialNB'):\n",
    "            classifier = MultinomialNB()\n",
    "        elif(classifierT == 'maxent'):\n",
    "            classifier = LogisticRegression()\n",
    "\n",
    "        initialize_new()\n",
    "\n",
    "        X, targets = fit_transform(train_data, train_pos_data)\n",
    "        train_y = targets\n",
    "        classifier.fit(X, train_y)\n",
    "\n",
    "        print(X.shape)\n",
    "        \n",
    "        X, targets = fit_transform(test_data, test_pos_data, fixed_features=True)\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        predictions = classifier.predict(X)\n",
    "        test_y = targets\n",
    "\n",
    "        # print(test_y.shape)\n",
    "        # print(predictions.shape)\n",
    "        # print(confusion_matrix(test_y, predictions))\n",
    "        confusionMatrix += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, average='binary')\n",
    "        p_score = precision_score(test_y, predictions, average='binary')\n",
    "        r_score = recall_score(test_y, predictions, average='binary')\n",
    "        \n",
    "        p_scores.append(p_score)\n",
    "        r_scores.append(r_score)        \n",
    "        scores.append(score)\n",
    "\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Precision Score:', sum(p_scores)/len(p_scores))\n",
    "    print('Recall Score:', sum(r_scores)/len(r_scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 583323)\n",
      "(1565, 583323)\n",
      "(4861, 580952)\n",
      "(1682, 580952)\n",
      "(4868, 563002)\n",
      "(1675, 563002)\n",
      "(4922, 573279)\n",
      "(1621, 573279)\n",
      "Score: 0.73806027188\n",
      "Precision Score: 0.767051730881\n",
      "Recall Score: 0.71167835531\n",
      "Confusion matrix:\n",
      "[[3484  543]\n",
      " [ 727 1789]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(df, pos_df, 'multinomialNB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 583322)\n",
      "(1565, 583322)\n",
      "(4861, 580951)\n",
      "(1682, 580951)\n",
      "(4868, 563002)\n",
      "(1675, 563002)\n",
      "(4922, 573278)\n",
      "(1621, 573278)\n",
      "Score: 0.825608135503\n",
      "Precision Score: 0.827634422406\n",
      "Recall Score: 0.823856576752\n",
      "Confusion matrix:\n",
      "[[3596  431]\n",
      " [ 444 2072]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(df, classifierT='maxent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-c3ff5902e04c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtry1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def try1():\n",
    "    yield 1, 2\n",
    "    yield 3, 4\n",
    "    \n",
    "def try2():\n",
    "    return [1, 2]\n",
    "\n",
    "for i, j in itertools.chain(try1(), try2()):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a,b;c.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'a,b;c.'.translate(string.punctuation)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', ';']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda c: c in s, string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi I\n",
      "I am\n",
      "am Varun\n",
      "Varun Raval\n",
      "hi I am\n",
      "I am Varun\n",
      "am Varun Raval\n"
     ]
    }
   ],
   "source": [
    "for s in _ngrams(nltk.word_tokenize('hi I am Varun Raval')):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = defaultdict()\n",
    "d.default_factory = None\n",
    "d[1] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 'PRP$'),\n",
       " ('friends', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " (',', ','),\n",
       " ('b', 'NN'),\n",
       " (',', ','),\n",
       " ('c', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('d', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize('my friends are a, b, c, and d.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using this classifier on another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/schemes/json', 'data/schemes/txt']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/schemes/txt/nodeset1783.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-1f6554524a0f>\u001b[0m in \u001b[0;36mread_files\u001b[1;34m(sources)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourceT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/schemes/txt/nodeset1783.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-66565088e23d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data/schemes/json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/schemes/txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_df1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-ed96ea8e2179>\u001b[0m in \u001b[0;36mget_df\u001b[1;34m(sources)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpos_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_non_args\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_args\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-1f6554524a0f>\u001b[0m in \u001b[0;36mread_files\u001b[1;34m(sources)\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourceT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'windows-1252'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/schemes/txt/nodeset1783.txt'"
     ]
    }
   ],
   "source": [
    "sources = ['data/schemes/txt']\n",
    "df1, pos_df1 = get_df(sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
