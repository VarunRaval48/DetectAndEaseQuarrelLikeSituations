{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas\n",
    "import json\n",
    "import nltk.data\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import string\n",
    "import array\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score, fbeta_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourceJ = 'data/araucaria/json'\n",
    "sourceT = 'data/araucaria/txt'\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def read_files():\n",
    "    for root, dir_names, file_names in os.walk(sourceJ):\n",
    "        for file_name in file_names:\n",
    "            file = open(os.path.join(sourceT, file_name[:-5]+'.txt'))\n",
    "            content = tokenizer.tokenize(file.read())\n",
    "            vals = []\n",
    "            for line in open(os.path.join(root, file_name)):\n",
    "                for node in json.loads(line)['nodes']:\n",
    "                    vals.append(node['text'])\n",
    "\n",
    "            r_val = [1]*len(vals)\n",
    "            args= []\n",
    "            non_args = []\n",
    "            for con in content:\n",
    "                is_args = False\n",
    "                i = 0\n",
    "                for val in vals:\n",
    "                    if val in con:\n",
    "                        is_args = True\n",
    "                        r_val[i] = 0\n",
    "                        break\n",
    "                    i+=1\n",
    "                if(is_args):\n",
    "                    args.append(con)\n",
    "                else:\n",
    "                    non_args.append(con)\n",
    "            \n",
    "            i = 0\n",
    "            for val in vals:\n",
    "                if(r_val[i] and val!='RA'):\n",
    "                    args.append(val)\n",
    "                i+=1\n",
    "                \n",
    "            yield file_name, args, non_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file = open('output.txt', 'w')\n",
    "for file_name, args, non_args in read_files():\n",
    "    s = file_name, ':', args, ':', non_args\n",
    "    file.write(str(s)+\"\\n\")\n",
    "\"\"\"\n",
    "\n",
    "headings = ['arguments', 'non arguments']\n",
    "index = []\n",
    "data = []\n",
    "for file_name, args, non_args in read_files():\n",
    "    index.append(file_name)\n",
    "    data.append([args, non_args])\n",
    "    \n",
    "df = pandas.DataFrame(index = index, data = data, columns = headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adverbs = ['RB', 'RBR', 'RBS']\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "verbs_remove = ['to be', 'to do', 'to have']\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    global word_list_1, word_list_2, word_list_3, adverb_list, verb_list, cnt_1, cnt_2, cnt_3, cnt_v, cnt_adv, cnt_c_2\n",
    "#     word_couples\n",
    "    word_list_1 = dict([])\n",
    "    word_list_2 = dict([])\n",
    "    word_list_3 = dict([])\n",
    "    adverb_list = dict([])\n",
    "    verb_list = dict([])\n",
    "#     word_couples = dict([])\n",
    "    cnt_1 = 0\n",
    "    cnt_2 = 0\n",
    "    cnt_3 = 0\n",
    "    cnt_v = 0\n",
    "    cnt_adv = 0\n",
    "    cnt_c_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    cnt_c_2 = 0\\n    length = len(words)\\n    for i in range(length-1):\\n        for j in range(i+1, length):\\n            word_c_2 = '{} {}'.format(words[i], words[j])\\n            if word_c_2 not in word_couples:\\n                word_couples[word_c_2] = cnt_c_2\\n                cnt_c_2 += 1\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize()\n",
    "def initialize_dicts_s(data):\n",
    "    global word_list_1, word_list_2, word_list_3, adverb_list, verb_list, word_couples, cnt_1, cnt_2, cnt_3, cnt_v, cnt_adv, cnt_c_2\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_list_1:\n",
    "            word_list_1[word] = cnt_1\n",
    "            cnt_1 += 1\n",
    "        if i>0:\n",
    "            word2 = '{} {}'.format(words[i-1], words[i])\n",
    "            if word2 not in word_list_2:\n",
    "                word_list_2[word2] = cnt_2\n",
    "                cnt_2 += 1\n",
    "        if i>1:\n",
    "            word3 = '{} {} {}'.format(words[i-2], words[i-1], words[i])\n",
    "            if word3 not in word_list_3:\n",
    "                word_list_3[word3] = cnt_3\n",
    "                cnt_3 += 1\n",
    "\n",
    "    cnt_adv = 0\n",
    "    cnt_v = 0\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in adverbs:\n",
    "            if word not in adverb_list:\n",
    "                adverb_list[word] = cnt_adv\n",
    "                cnt_adv += 1\n",
    "        if tag in verbs and word not in verbs_remove:\n",
    "            if word not in verb_list:\n",
    "                verb_list[word] = cnt_v\n",
    "                cnt_v += 1\n",
    "\"\"\"\n",
    "    cnt_c_2 = 0\n",
    "    length = len(words)\n",
    "    for i in range(length-1):\n",
    "        for j in range(i+1, length):\n",
    "            word_c_2 = '{} {}'.format(words[i], words[j])\n",
    "            if word_c_2 not in word_couples:\n",
    "                word_couples[word_c_2] = cnt_c_2\n",
    "                cnt_c_2 += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_dicts_all():\n",
    "    initialize()\n",
    "    for heading in headings:\n",
    "        for content in df[heading]:\n",
    "            for sentence in content:\n",
    "                if(sentence != ''):\n",
    "                    initialize_dicts_s(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    length = len(words)\\n    for i in range(length-1):\\n        for j in range(i+1, length):\\n            word_c_2 = '{} {}'.format(words[i], words[j])\\n            feature_couples[word_couples[word_c_2]] = 1\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_vector(sentence):\n",
    "    global word_list_1, word_list_2, word_list_3, adverb_list, verb_list, word_couples, cnt_1, cnt_2, cnt_3, cnt_v, cnt_adv, cnt_c_2\n",
    "\n",
    "    feature_1 = csr_matrix([0]*(len(word_list_1)))\n",
    "    feature_2 = csr_matrix([0]*len(word_list_2))\n",
    "    feature_3 = csr_matrix([0]*len(word_list_3))\n",
    "    feature_adv = csr_matrix([0]*len(adverb_list))\n",
    "    feature_v = csr_matrix([0]*len(verb_list))\n",
    "#     feature_couples = [0]*len(word_couples)\n",
    "    \n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for i, word in enumerate(words):\n",
    "        feature_1[0, word_list_1[word]] = 1\n",
    "        if word in adverb_list:\n",
    "            feature_adv[0, adverb_list[word]] = 1\n",
    "        if word in verb_list:\n",
    "            feature_v[0, verb_list[word]] = 1\n",
    "        if i>0:\n",
    "            word2 = '{} {}'.format(words[i-1], words[i])\n",
    "            feature_2[0, word_list_2[word2]] = 1\n",
    "        if i>1:\n",
    "            word3 = '{} {} {}'.format(words[i-2], words[i-1], words[i])\n",
    "            feature_3[0, word_list_3[word3]] = 1\n",
    "\n",
    "#     feature_couples = ''.join(map(str, feature_couples))\n",
    "#     feature = feature_1 + feature_2 + feature_3 + feature_adv + feature_v + feature_couples\n",
    "    feature = csr_matrix([feature_1, feature_2,  feature_3,  feature_adv,  feature_v])\n",
    "    return feature\n",
    "\n",
    "\"\"\"\n",
    "    feature_1 = hash(''.join(map(str, feature_1)))\n",
    "    feature_2 = hash(''.join(map(str, feature_2)))\n",
    "    feature_3 = hash(''.join(map(str, feature_3)))\n",
    "    feature_adv = hash(''.join(map(str, feature_adv)))\n",
    "    feature_v = hash(''.join(map(str, feature_v)))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    length = len(words)\n",
    "    for i in range(length-1):\n",
    "        for j in range(i+1, length):\n",
    "            word_c_2 = '{} {}'.format(words[i], words[j])\n",
    "            feature_couples[word_couples[word_c_2]] = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    initialize_dicts_all()\n",
    "    feature_vector_list = []\n",
    "    targets = []\n",
    "    for heading in headings:\n",
    "        for content in df[heading]:\n",
    "            for sentence in content:\n",
    "                if sentence != '':\n",
    "                    feature_vector_list.append(feature_vector(sentence))\n",
    "                    targets.append(heading)\n",
    "    \n",
    "    return feature_vector_list, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "PyEval_EvalFrameEx returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-96be19ef4866>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_vector_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-f4995254625f>\u001b[0m in \u001b[0;36mtrain_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0mfeature_vector_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                     \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheading\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-9f4bfdacf5c1>\u001b[0m in \u001b[0;36mfeature_vector\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#     feature_couples = ''.join(map(str, feature_couples))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#     feature = feature_1 + feature_2 + feature_3 + feature_adv + feature_v + feature_couples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mfeature_3\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mfeature_adv\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mfeature_v\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     67\u001b[0m                         self.format)\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# Read matrix dimensions given, if any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    173\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Simple -- other ideas?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set"
     ]
    }
   ],
   "source": [
    "feature_vector_list, targets = train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6702"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6702"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "a = str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _make_int_array():\n",
    "    return array.array(str(\"i\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _ngrams(tokens):\n",
    "    o_tokens = tokens\n",
    "    tokens = []\n",
    "    n_o_tokens = len(o_tokens)\n",
    "    \n",
    "    for n in range(2,4):\n",
    "        for j in range(n_o_tokens - n + 1):\n",
    "            yield ' '.join(o_tokens[j:j+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _couples(tokens):\n",
    "    length = len(tokens)\n",
    "    for i in range(length-1):\n",
    "        for j in range(i+1, length):\n",
    "            yield 'c_{} {}'.format(tokens[i], tokens[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _adverbs(pos_tags):\n",
    "    p = False\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in adverbs:\n",
    "            p = True\n",
    "            yield 'adv_' + word\n",
    "\n",
    "    if not p:\n",
    "        yield 'no_adverbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _verbs(pos_tags):\n",
    "    p = False\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in verbs and word not in verbs_remove:\n",
    "            p = True\n",
    "            yield 'v_' + word\n",
    "            \n",
    "    if not p:\n",
    "        yield 'no_verbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _modal_aux(pos_tags):\n",
    "    for word, tag in pos_tags:\n",
    "        if tag == 'MD':\n",
    "            return 'MD_1', 1\n",
    "    return 'MD_1', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sen_len(tokens):\n",
    "    yield 's_len', len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _avg_word_len(tokens):\n",
    "    c_len = 0\n",
    "    for token in tokens:\n",
    "        c_len += len(token)\n",
    "    yield 'avg_word_len', round(c_len/len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _punc_data(s):\n",
    "    punc_list = list(filter(lambda c: c in s, string.punctuation))\n",
    "    punc_len = len(punc_list)\n",
    "    yield 'punc_len', punc_len\n",
    "    single_punc = []\n",
    "    i = 0\n",
    "    while i < punc_len:\n",
    "        single_punc.append(punc_list[i])\n",
    "        last = punc_list[i]\n",
    "        i += 1\n",
    "        while(i < punc_len and punc_list[i] == last):\n",
    "            i += 1\n",
    "            \n",
    "    yield ''.join(single_punc), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new():\n",
    "    global features_\n",
    "    features_ = defaultdict()\n",
    "    features_.default_factory = features_.__len__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _feature_dict(sentence, fixed_features):\n",
    "    global features_\n",
    "    typeF = ''\n",
    "    if fixed_features == True:\n",
    "        features = dict(features_)\n",
    "        typeF = 'dict'\n",
    "    else:\n",
    "        features = features_\n",
    "        typeF = 'defaultDict'\n",
    "\n",
    "    feature_dict = {}\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    if len(tokens) > 2:\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        tag, val = _modal_aux(pos_tags)\n",
    "        if typeF == 'dict':\n",
    "            if tag in features:\n",
    "                feature_num = features[tag]\n",
    "                feature_dict[feature_num] = val\n",
    "        else:\n",
    "            feature_num = features[tag]\n",
    "            feature_dict[feature_num] = val\n",
    "\n",
    "        for tag, val in itertools.chain(_sen_len(tokens), _avg_word_len(tokens), _punc_data(sentence)):\n",
    "            if typeF == 'dict':\n",
    "                if tag in features:\n",
    "                    feature_num = features[tag]\n",
    "                    feature_dict[feature_num] = val\n",
    "            else:\n",
    "                feature_num = features[tag]\n",
    "                feature_dict[feature_num] = val\n",
    "\n",
    "        for token in itertools.chain(_adverbs(pos_tags), _verbs(pos_tags), tokens, _ngrams(tokens), _couples(tokens)):\n",
    "            if typeF == 'dict':\n",
    "                if token in features:\n",
    "                    feature_num = features[token]\n",
    "                    if feature_num not in feature_dict:\n",
    "                        feature_dict[feature_num] = 1\n",
    "                    else:\n",
    "                        feature_dict[feature_num] += 1\n",
    "            else:\n",
    "                feature_num = features[token]\n",
    "                if feature_num not in feature_dict:\n",
    "                    feature_dict[feature_num] = 1\n",
    "                else:\n",
    "                    feature_dict[feature_num] += 1\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_transform(dataFrame, fixed_features=False):\n",
    "    global features_\n",
    "    j_indices = []\n",
    "    indptr = _make_int_array()\n",
    "    values = []\n",
    "    indptr.append(0)\n",
    "    targets = []\n",
    "    for i, heading in enumerate(headings):\n",
    "        for content in dataFrame[heading]:\n",
    "            for sentence in content:\n",
    "                feature_dict = _feature_dict(sentence, fixed_features)\n",
    "                # print(sentence, feature_dict)\n",
    "                if len(feature_dict) != 0:\n",
    "                    targets.append(i)\n",
    "                    j_indices.extend(feature_dict.keys())\n",
    "                    values.extend(feature_dict.values())\n",
    "                    indptr.append(len(j_indices))\n",
    "\n",
    "    j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "    indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "    targets = np.asarray(targets, dtype=np.intc)\n",
    "    X = csr_matrix((values, j_indices, indptr), shape = (len(indptr) - 1, len(features_)), dtype=np.int64)\n",
    "    return X, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initialize_new()\n",
    "X, targets = fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6543"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6543, 701401)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 23,  5, ...,  1,  1,  1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arguments        [Indonesia cannot afford to become a haven for...\n",
       "non arguments                                                   []\n",
       "Name: nodeset190.json, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df.iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indonesia cannot afford to become a haven for an Islamic radicalism which would wreck the economy and tear the archipelago apart.',\n",
       " 'The war on terror will be intensified.',\n",
       " 'If Indonesia wants to avoid becoming a haven for Islamic radicalism the war on terror has to be intensified',\n",
       " 'There is a possibility for Indonesia to become a haven for Islamic radicalism',\n",
       " 'which would wreck the economy and tear the archipelago apart.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['arguments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{8} During the last thirty years, scientists have discovered that the existence of intelligent life depends upon a complex and delicately balanced set of initial conditions simply given in the Big Bang itself.',\n",
       " 'We now know that life-prohibiting universes are vastly more probable than life- permitting universes like ours.',\n",
       " 'How much more probable?',\n",
       " 'Well, before I give you an estimation, let me just give you some numbers to give you a feel for the odds.',\n",
       " \"The number of seconds in the history of the universe is about 1018, that's ten followed by eighteen zeros.\",\n",
       " 'The number of subatomic particles in the entire universe is about1080.',\n",
       " 'Now with those numbers in mind, consider the following.',\n",
       " \"Donald Page, one of America's eminent cosmologists, has calculated the odds of our universe existing as on the order of one chance out of 1010(123), a number which is so inconceivable that to call it astronomical would be a wild understatement!\",\n",
       " '{9} Robert Jastrow, the head of NASA\\'s Goddard Institute for Space Studies, has called this the most powerful evidence for the existence of God \"ever to come out of science.',\n",
       " '\"{10} Once again, the view that Christian theists have always held, that there is an intelligent designer of the Cosmos, seems to me to be much more plausible than the atheistic interpretation of chance.',\n",
       " 'http://www.origins.org/articles/craig_washington_3.html']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['non arguments'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# j_indices = []\n",
    "# values = []\n",
    "# indptr = _make_int_array()\n",
    "# indptr.append(0)\n",
    "\n",
    "# feature_dict = _feature_dict('Indonesia cannot afford to become a haven for an Islamic radicalism which would wreck the economy and tear the archipelago apart.')\n",
    "# j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "# indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "# targets = np.asarray(targets, dtype=np.intc)\n",
    "\n",
    "X, targets = fit_transform(df.iloc[[0]])\n",
    "\n",
    "classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 701401)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kFoldTest(data, classifierT = 'multinomialNB'):\n",
    "#     pipeline = Pipeline([\n",
    "#             ('vectorizer', Vectorize()),\n",
    "#             ('classifier', classifier)\n",
    "#         ])\n",
    "\n",
    "    kFold = KFold(n = len(data), n_folds = 4)\n",
    "    scores = []\n",
    "    p_scores = []\n",
    "    r_scores = []\n",
    "    confusionMatrix = np.array([[0]*2]*2)\n",
    "#     vectorizer = Vectorize()\n",
    "    #classifier = MultinomialNB()\n",
    "    #classifier = BernoulliNB()\n",
    "    #classifier = SVC()\n",
    "    #classifier = RandomForestClassifier()\n",
    "    for train_indices, test_indices in kFold:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        test_data = data.iloc[test_indices]\n",
    "\n",
    "        if(classifierT == 'multinomialNB'):\n",
    "            classifier = MultinomialNB()\n",
    "        elif(classifierT == 'maxent'):\n",
    "            classifier = LogisticRegression()\n",
    "\n",
    "        initialize_new()\n",
    "\n",
    "        X, targets = fit_transform(train_data)        \n",
    "        train_y = targets\n",
    "        classifier.fit(X, train_y)\n",
    "\n",
    "        print(X.shape)\n",
    "        \n",
    "        X, targets = fit_transform(test_data, fixed_features=True)\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        predictions = classifier.predict(X)\n",
    "        test_y = targets\n",
    "\n",
    "        # print(test_y.shape)\n",
    "        # print(predictions.shape)\n",
    "        # print(confusion_matrix(test_y, predictions))\n",
    "        confusionMatrix += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, average='binary')\n",
    "        p_score = precision_score(test_y, predictions, average='binary')\n",
    "        r_score = recall_score(test_y, predictions, average='binary')\n",
    "        \n",
    "        p_scores.append(p_score)\n",
    "        r_scores.append(r_score)        \n",
    "        scores.append(score)\n",
    "\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Precision Score:', sum(p_scores)/len(p_scores))\n",
    "    print('Recall Score:', sum(r_scores)/len(r_scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 583322)\n",
      "(1565, 583322)\n",
      "(4861, 580951)\n",
      "(1682, 580951)\n",
      "(4868, 563002)\n",
      "(1675, 563002)\n",
      "(4922, 573278)\n",
      "(1621, 573278)\n",
      "Score: 0.73806027188\n",
      "Precision Score: 0.767051730881\n",
      "Recall Score: 0.71167835531\n",
      "Confusion matrix:\n",
      "[[3484  543]\n",
      " [ 727 1789]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest('multinomialNB', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 583322)\n",
      "(1565, 583322)\n",
      "(4861, 580951)\n",
      "(1682, 580951)\n",
      "(4868, 563002)\n",
      "(1675, 563002)\n",
      "(4922, 573278)\n",
      "(1621, 573278)\n",
      "Score: 0.825608135503\n",
      "Precision Score: 0.827634422406\n",
      "Recall Score: 0.823856576752\n",
      "Confusion matrix:\n",
      "[[3596  431]\n",
      " [ 444 2072]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(df, classifierT='maxent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-c3ff5902e04c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtry1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def try1():\n",
    "    yield 1, 2\n",
    "    yield 3, 4\n",
    "    \n",
    "def try2():\n",
    "    return [1, 2]\n",
    "\n",
    "for i, j in itertools.chain(try1(), try2()):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a,b;c.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'a,b;c.'.translate(string.punctuation)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', ';']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda c: c in s, string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi I\n",
      "I am\n",
      "am Varun\n",
      "Varun Raval\n",
      "hi I am\n",
      "I am Varun\n",
      "am Varun Raval\n"
     ]
    }
   ],
   "source": [
    "for s in _ngrams(nltk.word_tokenize('hi I am Varun Raval')):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = defaultdict()\n",
    "d.default_factory = None\n",
    "d[1] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
